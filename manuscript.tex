\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

\makeatletter
\let\NAT@parse\undefined
\makeatother


\def\pub{false} % true for publication, false for draft
\newcommand*{\template}{dding_template}
\input{\template/preamble/preamble_conf.tex}


\title{\LARGE \bf
Imposing a Weight Norm Constraint for Neuro–Adaptive Control
}

\author{Myeongseok Ryu$^{1}$, Jiyun Kim$^{2}$, and Kyunghwan Choi$^{1}$% <-this % stops a space
\thanks{*This work was supported by a Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (P0020535, The Competency Development Program for Industry Specialist) \it{(Corresponding author: Kyunghwan Choi)}}% <-this % stops a space
\thanks{$^{1}$Myeongseok Ryu and Kyunghwan Choi are with the School of Mechanical and Robotics Engineering, Gwangju Institute of Science and Technology, 61005 Gwangju, Republic of Korea {\tt\small dding\_98@gm.gist.ac.kr, khchoi@gist.ac.kr}}%
\thanks{$^{2}$Jiyun Kim is with the AI Graduate School, Gwangju Institute of Science and Technology, 61005 Gwangju, Republic of Korea 
        {\tt\small jiyun6606@gm.gist.ac.kr}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

In this paper, a neuro–adaptive controller with weight norm constraints is proposed for uncertain Euler‒Lagrange systems. 
The boundedness of the weights in the neuro–adaptive controller design is important to prevent excessively large control inputs and system instability. 
To ensure the boundedness of the weights, the weight norm constraints are imposed as inequality constraints in the weight adaptation. 
The adaptation law is derived based on the constrained optimization method. 
The stability of the proposed controller is analyzed in the sense of Lyapunov, ensuring the boundedness of the tracking error and weight estimation. 
For the comparative study, two benchmark controllers and the proposed controller were evaluated through a numerical simulation of a two-link manipulator system and compared in terms of tracking performance and parameter dependency. 
The comparative study verified that the proposed controller has better tracking performance and lower parameter dependency.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  SECTION INTRODUCTION ===================================

\section*{Notation}
In this study, the following notation is used:

\begin{itemize}
    \item $\otimes$ denotes the Kronecker product \cite[Definition 7.1.2]{Bernstein:2009aa}.
    \item $\mv x=[x_i]_{i\in\{1,\cdots,n\}}\in\R^n$ denotes a vector.
    % \item $\mv x=[x_i]_{i\in[1,\cdots,n]}\in\R^n$ and $
    %     \mm A
    %     := 
    %     [a_{ij}]
    %     _{
    %         i\in[1,\cdots,n],j\in[1,\cdots ,]
    %     }\allowbreak\in\R^{n\times m}
    %     $ denotes a vector and a matrix.
    \item $\myrow_i(\mm A)$ denotes the $i\textsuperscript{th}$ row of the matrix $\mm A\in\R^{n\times m}$. 
    \item $\myvec(\mm A):= [\myrow_1(\mm A^\top)  ,\cdots,\myrow_m(\mm A^\top)  ]^\top   $ for $\mm A\in\R^{n\times m}$.
    % \item $\text{col}_i(A)$ and $\myrow_j(A)$ denote the $i\textsuperscript{th}$ column and $j\textsuperscript{th}$ row of $A\in\R^{n\times m}$, respectively.
    % \item $\myvec(A):= [\text{col}_1(A)^\top  ,\cdots,\text{col}_m(A)^\top  ]^\top   $ for $A\in\R^{n\times m}$.
    \item $\lambda_\text{min}(\mm A)$ denotes the minimum eigenvalue of the matrix $\mm A\in\R^{n\times n}$.
    \item $\mm I_n$ denotes the $n\times n$ identity matrix and $\mm 0_{n\times m}$ denotes the $n\times m$ zero matrix.
\end{itemize}

\section{Introduction} \label{sec:intro}

In various practical applications, Euler‒Lagrange systems generally have uncertainties due to their unknown and unmodeled dynamics.
These uncertainties degrade the control performance index.
Furthermore, if uncertainties dominate the system, they may lead to instability.
By compensating for these uncertainties, adaptive control methods have been widely used to attenuate the effects of system uncertainties \cite{Tao:2003aa,Ioannou:2006aa}.
Most conventional adaptive control methods focus on estimating unknown parameters in systems or controllers.

More recently, neuro–adaptive control approaches have been developed using neural networks (NNs) to approximate unknown system dynamics \cite{Farrell:2006aa}.
It generally uses the approximation capability of NNs with any arbitrary sigmoidal activation function \cite{Cybenko:1989aa}, which can approximate sufficiently smooth functions with minimum approximation error through ideal weights in a compact set.
This property allows the NNs to be synthesized in the controllers to approximate the unknown dynamics.
Various architectures of NNs have been proposed for neuro–adaptive control, such as single-hidden-layer NN (SHLNN) \cite{Esfandiari:2015aa,Gao:2006aa}, radial basis function NN (RBFNN) \cite{Ge:2002aa,Zhou:2023aa} and deep NN (DNN) \cite{Patil:2022aa}.
The literature has shown that NNs can improve the system's performance index and stability by approximating the unknown dynamics.

% The corresponding issue in the adaptive control is the parameter drift.
% It refers to the weight divergence during the adaptation process, while the tracking error is bounded.

% One of the common issues in neuro–adaptive control is that the boundedness of the control inputs is not guaranteed.
One of the common issues in using NNs 
\color{red}
for control compontnets
\color{black}
is that the outputs of NNs are not predictable.
This is because the input‒output relationships of the NNs are not interpretable (\ie the NNs are usually called black boxes \cite{Sheu:2020aa,Rudin:2019aa}).
\color{red}
This issue should be addressed for safety because the control input may reach the actuator's physical limitations and result in system collapse, due to the unpredictable control conponents from NNs.
\color{black}
\st{
    This issue should be addressed for safety because the unpredictable control input may reach the actuator's physical limitations and result in system collapse.
}

Most studies modify their adaptation laws to ensure the boundedness of the weights, noting that the boundedness of the NNs' output can be ensured by the boundedness of the weights.
In \cite{Zhou:2023aa,Patil:2022aa}, the projection operator is utilized to prevent weight divergence by projecting the adaptation direction on the convex set of weights.
However, in the literature, the convex set is usually selected to be as large as possible since there is no information on the ideal weights' norm.
Hence, the projection operator guarantees only the boundedness of the weights without theoretical optimality.
In addition, the $\sigma$-modification\cite{Ge:2002aa} and the $\epsilon$-modification \cite{Esfandiari:2015aa,Gao:2006aa} are widely used to regulate the magnitude of the weights by adding a stabilizing function in the adaptation law, therby making the invariance set of the estimation error of the weights. 
The existing methods have shown their effectiveness in ensuring the boundedness of the weights via numerical simulations.
However, they lack theoretical analysis regarding the optimality of the adapted weights.

Similarly, approaches that regulate the magnitude of the weights have also been introduced in the deep learning literature.
One of the approaches is $L_2$-regularization, which adds the squared magnitudes of the weights to the objective function \cite{Wu:2020aa,Lewkowycz:2020aa}.
Then, the adaptation process attempts to reduce the magnitude of the weights, not only the original objective function.
By regulating the magnitude of the weights, the stability of the adaptation process can be enhanced, and overfitting can be prevented.
However, $L_2$-regularization also involves a trade-off between adaptation stability and the optimality of the weights.

For the theoretical analysis of weight optimality with boundedness, the constrained optimization method \cite{Nocedal:2006aa} can be utilized.
It provides the theoretical definition of optimality and the numerical methodology to solve the constrained optimization problem.
To the best of the authors’ knowledge, no prior work has utilized the constrained optimization method for the real-time weight adaptation of neuro–adaptive control satisfying weight boundedness.
Therefore, the constrained optimization method could play a pivotal role in the neuro–adaptive control design addressing the weight boundedness.

The main contributions of this study are as follows:
\begin{itemize}
    \item The neuro–adaptive control problem is reformulated into a constrained optimization problem by treating the satisfaction of the weight boundedness as inequality constraints.
    \item The adaptation law is derived based on the constrained optimization method to minimize the objective function while satisfying the weight norm constraints.
    \item The stability of the adaptation laws is analyzed via Lyapunov stability analysis, which ensures the boundedness of the tracking error and weight estimation.
\end{itemize}

The remainder of this paper is organized as follows:
Section \ref{sec: Problem Formulation} presents the target-constrained system and control objective.
Section \ref{sec:ctrl design} introduces the proposed controller and the adaptation law. 
Section \ref{sec:stability} examines the stability of the proposed controller.
A comparative study of the three selected controllers, including the proposed controller through numerical simulation, is reported in Section \ref{sec:sim}.
Finally, Section \ref{sec:conclusion} concludes the paper by presenting future work.

%  SECTION PROBLEM FORMULATION ============================
\section{Problem Formulation}\label{sec: Problem Formulation}

\subsection{Model Dynamics and Control Objective}

Consider an uncertain Euler‒Lagrange system modeled as
\begin{equation}
    \mm M\ddq + \mm V_m\dq+ \mm G + \mm F 
    =
    \mv\tau 
    ,
    \label{eq. system dynamics 1}
\end{equation}
where $\q\in \mathbb{R}^n$ and $\mv \tau\in\R^n$ denote the generalized coordinate and the control input, respectively; $\mm M :=\mm M(\q)\in\mathbb{R}^{n\times n}$, $\mm V_m:= \mm V_m(\q,\dq)\in\mathbb{R}^{n\times n}$, and $\mm G:=\mm G(\q)\in\mathbb{R}^{n}$ denote the unknown system function matrices; and $\mm F:=\mm F(\dq)\in\mathbb{R}^{n}$ denotes the external force.
Using the user-designed matrices $\widehat{\mm M}>0,\widehat{\mm V}_m$ and $\widehat{\mm G}$, \eqref{eq. system dynamics 1} can be represented as 
\begin{equation}
    \widehat{\mm M}\ddq+\widehat{\mm V}_m\dq+\widehat{\mm G} = \mv \tau + \mv f(\q,\dq,\ddq)
    ,
    \label{eq. system dynamics 2}
\end{equation}
where $\mv f(\q,\dq,\ddq) := -(\mm M-\widehat{\mm M})\ddq-(\mm V_m-\widehat{\mm V}_m)\dq -(\mm G-\widehat{\mm G}) -\mm F$ denotes the residual unknown term.

Hence, the objective of the control design is to make $\q$ track the continuously differentiable desired trajectory $\qd:=\qd(t):\R\to \R^n$ under the unknown terms $\mv f:=\mv f(\q,\dq,\ddq)$.

%  SECTION CONTROLLER DESIGN ==============================
\section{Control Law Development}\label{sec:ctrl design}

In this section, the neuro–adaptive controller is developed.
Section \ref{sec:ctrl dev} presents the details of the neuro–adaptive controller and the NN model.
The adaptation law based on the constrained optimization method is derived in Section \ref{sec:adap_laws} by formulating a constrained optimization problem.

\subsection{Neuro–adaptive Control Design} \label{sec:ctrl dev}

The backstepping control-based approach is utilized to generate a reference signal $\mv z^*:= -{k_q}\tilde {\q}+\dq_d$ for $\mv {z}:= \dq$, where $\tilde {\q}:= \mv {q} - \qd$ and $k_q\in\R_{>0}$.
The desired stabilizing controller can be designed as follows:
\begin{equation}
    \mv \tau^* = 
    -\widehat{\mm M}\cdot ({k_z}\tilde {\mv z})
    + 
    ( 
        -\widehat{\mm M}\tilde {\q}
        +
        \widehat{\mm V}_m{\mv z}
        +
        \widehat{\mm G}
        -
        \mv f
        +
        \widehat{\mm M} \dot {\mv z}^*
    ) 
    ,
    \label{eq. desired control}
\end{equation}
where $\tilde {\mv z}:= \mv z-\mv z^*$ and $k_z\in\R_{>0}$.
Note that the desired controller cannot be realized because of $\mv f$.

To approximate the desired controller, an NN is utilized.
\color{red}
    Even though the DNNs have a higher approximation capability than the SHLNNs \cite{Rolnick:2017aa}, here, the SHLNN is utilized for simplicity and low computational complexity.
\color{black}
The NN with a single hidden layer is represented as 
\begin{equation}
    \NN({\q_{n}};\wth):= \wV_1^\top \act(\wV_0^\top {\q_{n}})
    ,
\end{equation}
where ${\q_{n}}\in\R^{l_0+1}$ denotes the NN input vector, $\wV_i\in\R^{(l_i+1)\times l_{i+1}},\ \forall i\in\{0,1\}$ denotes the weight matrix of the $i\textsuperscript{th}$ layer and $\act:\R^{l_1}\to\R^{l_1+1}$ denotes the activation function layer.
The activation function layer consists of an elementwise nonlinear function $\sigma(\cdot)$ and an augmented $1$ to combine the bias terms in the weight matrix (\ie $\act(\mv x) = \left(\sigma(\mv x_{(1)}),\cdots, \sigma(\mv x_{(l_1)}), 1\right)^\top $).
For further simplicity, let $\wth_i:= \myvec(\wV_i)\in\R^{\Xi_i},\ \forall i\in\{0,1\}$ denotes the vectorized weights and $\wth:=(\wth_1^\top ,\wth_0^\top )^\top \in\R^{\Xi}$ denote the total weight vector, where $\Xi_i:=(l_i+1)\cdot l_{i+1},\ \forall i\in\{0,1\}$ and $\Xi:=\Xi_0+\Xi_1$ denote the number of each layer and total weights, respectively.

Using this NN, the desired controller $\mv \tau^*$ can be approximated by the ideal weight vector $\wth^*$ for a compact subset $\Omega_{NN}\in\R^{l_0+1}$ to $\epsilon$ accuracy such that $\sup_{{\q_{n}}\in\Omega_{NN}} \allowbreak \norm{\NN({\q_{n}};\wth^*) - \mv \tau^*}=\epsilon<\infty$ \cite{Cybenko:1989aa}.
The ideal weight vector $\wth^*$ is typically assumed to be bounded.
Then, using the estimated weight vector $\estwth:=(\estwth_1^\top ,\estwth_0^\top )^\top $ of $\idealwth:=({\idealwth_{1}}^{\top},{\idealwth_{0}}^{\top})^\top $ and bounded approximation error $\mv \epsilon\in\R^2$, the approximated desired controller $\mv \tau^*\approx -\NN(\q_{n};\idealwth)-\mv\epsilon$ can be estimated as follows:
\begin{equation}
    \mv \tau := -\NN({\q_{n}};\estwth)
    .
    \label{eq. control law}
\end{equation}
For further sections, let $\idealNN:=\NN({\q_{n}};\idealwth)$ and $\idealact :=\act({\idealwV_0}^{\top}{\q_{n}})$, and let $\estNN:={\NN}({\q_{n}};\estwth)$, $\estact :=\act(\estwV_0^{\top}{\q_{n}})$ and $\estact' := \pptfrac{\estact}{(\estwV_0^\top \q_{n})}$.

Using \eqref{eq. system dynamics 2}, \eqref{eq. desired control}, and \eqref{eq. control law}, the error dynamics can be obtained as the first-order system of augmented error $\mv \xi:=({\tilde {\q}}^\top ,{\tilde {\mv z}}^\top)^\top \in\R^{2n}$ as follows:
\begin{equation}
    \ddtt{\mv \xi} 
    = 
    \mm A_\xi{\mv \xi}+\mm B_\xi(\NN^*-\estNN+\mv\epsilon)
    ,
\end{equation}
and 
\begin{equation}
    \mm A_\xi := 
    \begin{bmatrix}
        -{k_q} \mm I_{n} &\mm I_{n}
        \\
        -\mm I_{n}& -{k_z} \mm I_{n}
    \end{bmatrix}
    ,\ 
    \mm B_\xi := 
    \begin{bmatrix}
        \mm 0_{n\times n}\\\widehat{\mm M}^{-1}
    \end{bmatrix}.
\end{equation}

\subsection{Adaptation Law Derivation}\label{sec:adap_laws}

\color{red}
    As discussed in Section \ref{sec:intro}, the boundedness of the weights should be considered to prevent excessively large control inputs and system instability.
\color{black}
For the boundedness of the weights, weight norm constraints are imposed on the adaptation process such that $c_{\theta_i}:=c_{\theta_i}(\estwth_i)=\norm{{\estwth}_i}^2-\bar\theta^2_i,\ \forall i\in\{0,1\}$.
The control problem can be reformulated into a constrained optimization problem as follows:
\begin{equation}
    \begin{matrix}
        \min_{{\estwth}} \ J(\mv \xi;{\estwth})
        :=
        \tfrac{1}{2}
        \mv \xi^\top \mm \Lambda \mv \xi
        \\ \\
        \begin{aligned}
        \text{subject to\;}&c_{j}\le0, \quad j\in\mathcal I 
        := 
        \{\theta_0,\theta_1\}
        ,
        \end{aligned}
    \end{matrix}
\end{equation}
where $\mm \Lambda=\mm \Lambda^\top >0$ denotes the weighting matrix.
In this optimization problem, $\mv \xi$ is considered a predefined parameter.
The corresponding Lagrangian function is defined as
\begin{equation}
    L(
        \mv \xi,
        {\estwth},
        [\lambda_j]_{j\in\mathcal I}
    )
    := 
    J(\mv \xi;{\estwth})
    +
    \textstyle\sum_{j\in\mathcal I} \lambda_jc_j({\estwth})
    .
\end{equation}
% where $\mathcal I=\{j\ \vert \ c_j\ge 0\}$ denotes the active set.
To solve the dual problem $\min_{{\estwth}} \max_{[\lambda_j]_{j\in\mathcal I}}  L(\mv \xi,{\estwth},[\lambda_j]_{j\in\mathcal I}$, the adaptation law is derived as follows:
\begin{subequations}
    \begin{align}
            \ddtt {{\estwth}}
            &
            =
            -\alpha 
            \pptfrac{L}{{\estwth}}
            =-\alpha 
            \left(
                \pptfrac{J}{{\estwth}}
                +
                \textstyle\sum_{j\in\mathcal{I}}
                \lambda_j 
                \pptfrac{c_j}{\estwth}
            \right),
        \label{eq. adaptation law th}
            \\
            \ddtt\lambda_j
            & 
            = 
            \beta_j
            \pptfrac{L}{\lambda_j} 
            = 
            \beta_j c_j ,
            \quad\quad\quad\quad      \      
            \forall j\in\mathcal I,
        \label{eq. adaptation law L}
            \\
            \lambda_j 
            & 
            = 
            \max(\lambda_j,0) ,
        \label{eq. adaptation law L max}
    \end{align}
    \label{eq. adaptation laws}
\end{subequations}
where arguments of $L$ are suppressed for brevity, and $\alpha\in\R_{>0}$ and $\beta_j\in\R_{>0}$ denote the adaptation gain and the update rate for each Lagrange multiplier, respectively.
\color{red}
% The Eq.~\eqref{eq. adaptation law L max} is analog to using an active set $\mathcal I=\{j\ \vert \ c_j\ge 0\}$ in constrained optimization theory which excludes terms of satisfied constraints in adaptation law.
\color{black}

Using the chain rule, the gradient of the objective function with respect to the weights (\ie $\pptfrac{J}{\estwth}$) in \eqref{eq. adaptation law th} can be represented as
% \begin{equation}
$
    \pptfrac{J}{\estwth}
    =
    \pptfrac{\mv\xi}{\estwth}^\top 
    \mm \Lambda
    \mv\xi
    .
% \end{equation}
$
The calculation of $\pptfrac{J}{\estwth}$ is not straightforward because of the dynamics of $\mv\xi$.
Using the forward sensitivity method presented in \cite{Sengupta:2014aa}, the sensitivity equation can be obtained as follows:
\begin{equation}
    \ddtt{\mm \eta} =
    \mm A_\xi \mm\eta
    - \mm B_\xi 
    % (\partial \estNN/\partial {\estwth})
    \pptfrac{\estNN}{\estwth}
    ,
    \label{eq. sen eq}
\end{equation}
where ${\mm \eta}:= \pptfrac{\mv\xi}{\estwth}\in\R^{2n\times \Xi}$ denotes the sensitivity of the weights to the augmented error.
The initial value of ${\mm \eta}$ is zero since the initial $\mv\xi$ is independent of the weights.
By decomposing for each layer, the dynamics of ${\mm \eta}_i:= \pptfrac{\mv\xi}{\estwth}_i\in\R^{2n\times \Xi_i}$ can be represented as
\begin{equation}
    \begin{aligned}
        \ddtt{\mm \eta} 
        \!=\!&
        \begin{bmatrix}
            {\mm \eta}_1&{\mm \eta}_0
        \end{bmatrix}'
        \\
        \!=\!&
        \mm A_\xi
        % \begin{bmatrix}
        [
            {\mm \eta}_1
            \ \ 
            {\mm \eta}_0
        ]
        % \end{bmatrix}
        \!-\!
        \mm B_\xi
        % \begin{bmatrix}
        [
            (\mm I_{l_2}\!\otimes\!\estact^\top )
            \ \ 
            \estwV_1^\top \estact'(\mm I_{l_1}\otimes {\q_{n}}^\top )
        ]
        % \end{bmatrix}
        .
    \end{aligned}
\end{equation}
The calculation of $\pptfrac{\estNN}{\estwth}$ is introduced in \cite{Patil:2022aa}.
In conclusion, the gradient of the objective with respect to the weights can be obtained as $\pptfrac{J}{\estwth} = {\mm \eta}^\top\mm \Lambda\mv\xi$ by simulating the sensitivity equation \eqref{eq. sen eq}.

On the other hand, the gradient of the constraints with respect to the weights can be represented as follows:
\begin{equation}
    \pptfrac{c_{\theta_0}}{{\estwth}}
    = 
    \begin{bmatrix}
        \mm 0_{\Xi_1\times 1} \\
        2{\estwth}_0 
    \end{bmatrix}
    ,\quad
    \pptfrac{c_{\theta_1}}{{\estwth}}    
    = 
    \begin{bmatrix}
        2{\estwth}_1 \\
        \mm 0_{\Xi_0\times 1}
    \end{bmatrix}
    .
\end{equation}

%  SECTION STABILITY ANALYSIS ==============================
\section{Stability Analysis}\label{sec:stability}

The following theorem proves the boundedness of the tracking error and the weight estimation of the weights.

\begin{theorem}
For the dynamical system in \eqref{eq. system dynamics 1}, the proposed controller \eqref{eq. control law} and the adaptation law \eqref{eq. adaptation laws} ensure the boundedness of the tracking error $\mv \xi$ and the weight estimation ${\estwth}$, provided that the control gains ${k_q}$ and ${k_z}$ satisfy \eqref{eq. ctrl stable condition}.
\end{theorem}

\begin{proof}
The boundedness is proven from the last layer to the first layer.

\subsection*{Step 1: Boundedness of ${\estwth}_1,{\mm \eta}_1,\mv \xi$}

% For convenience, assume that all the constraints are in the active set without loss of generality.
% If the constraints are not in the active set, the boundedness cannot be guaranteed, but weights will be adapted to reduce the objective function until the constraints are violated.
\color{black}
Without loss of generality, assume that all the constraints are violated. 
Then, according to \eqref{eq. adaptation law L} and \eqref{eq. adaptation law L max}, all Lagrange multipliers are nonzero.
%If the Lagrange multipliers are not sufficiently generated, the boundedness cannot be guaranteed, but weights will be adapted to reduce the objective function until the constraint violations.
\color{black}

The dynamics of $\mv \xi$ can be represented as
\begin{equation}    
    \ddtt {\mv\xi} = \mm A_\xi\mv \xi +\mm B_\xi
    (
        -\estwV_1^\top \estact+\mv w(t)
    )
    ,
\end{equation}
where $\mv w(t):= {\idealwV_1}^{\top}\idealact+\mv \epsilon$ is a lumped residual term, which is bounded as $\norm{\mv  w(t)}\le \bar w< 0$ since $\norm{\idealwth_1},\norm{\idealact}$ and $\norm{\mv\epsilon}$ are bounded.
The dynamics of ${\mm \eta}_1$ and ${\estwth}_1$ are represented as
\begin{equation}
    \begin{aligned}
        \ddtt{\mm \eta}_1 =
        & 
        \mm A_\xi {\mm \eta}_1 -\mm B_\xi (\mm I_{l_{2}}\otimes \estact^\top )
        ,
        \\
        \ddtt{{\estwth}}_1 =
        & -\alpha 
        (
            {\mm \eta}_1^\top 
            \mm \Lambda
            \mv\xi
            +
            2\lambda_{\theta_1} {\estwth}_1
        )
        .
    \end{aligned} 
\end{equation}
According to \cite[Chap.~4 T.~1.9]{Desoer:2009aa}, the boundedness of ${\mm \eta}_1$ can be obtained since $\mm A_\xi$ is stable and the residual term $\norm{-\mm B_\xi (\mm I_{l_{2}}\otimes \estact^\top )}$ is bounded.

Define the Lyapunov function $V_1:=\tfrac{1}{2}\mv\xi^\top \mm P\mv\xi+\tfrac{1}{2\alpha}{\estwth}_1^\top {\estwth}_1$, with the Lyapunov equation $\mm A_\xi^\top \mm P+\mm P\mm A_\xi=-\mm Q$, where $\mm A_\xi<0,\mm P=\mm P^\top >0$, and $\mm Q>0$.
Using a proposition $\estwV_1^\top \estact = \myvec(\estwV_1^\top \estact)=\myvec(\estact^\top \estwV_1) = (\mm I_{l_2}\otimes {\estact}^\top )\myvec(\estwV_1)=(\mm I_{l_2}\otimes \estact^\top ){\estwth}_1$ \cite[Proposition~(7.1.9)]{Bernstein:2009aa}, the time derivative of $V_1$ is
\begin{equation}
    \begin{aligned}
        \ddtt \! {V}_1 
        \!=\!& 
        \tfrac{1}{2}
        \mv \xi^\top 
            (
                \mm A_\xi^\top \mm P+\mm P\mm A_\xi
            )
        \mv \xi
        \!+\!
        \mv \xi^\top \mm P (
            \!-\!\mm B_\xi\estwV_1^\top \estact +\mm B_\xi \mv w(t) 
        )\\
        &
        -{\estwth}_1^\top 
        \!
        (
            {\mm \eta}_1^\top \mm \Lambda \xi+2\lambda_{\theta_1} {\estwth}_1 
        )
        \\
        =& 
        -\tfrac{1}{2}
        \mv \xi^\top \mm Q\mv \xi 
        -
        \mv \xi^\top \mm P\mm B_\xi(\mm I_{l_2}\otimes \estact^\top ){\estwth}_1 
        +\mv \xi^\top \mv \Delta
        \\
        &
        -{\estwth}_1^\top {\mm \eta}_1^\top \mm \Lambda\mv \xi
        -2\lambda_{\theta_1} {\estwth}_1^\top {\estwth}_1
        \\
        \le&
        -
        \tfrac{1}{2}
        \lambda_\text{min}(\mm Q)\norm{\mv \xi} ^2
        +\bar\Delta \norm{\mv \xi}
        +\bar M\norm{\mv \xi} \norm{{\estwth}_1}
        \\
        &
        -2\lambda_{\theta_1}
        \norm{{\estwth}_1}^2
        \\
        \le& 
        (
            -\tfrac{1}{2}\lambda_\text{min}(\mm Q)
            +{\tfrac{\bar M}{2}}
        )
        \norm{\mv \xi}^2 +\bar\Delta \norm{\mv \xi}
        \\
        &+ 
        (
        -2\lambda_{\theta_1} 
        +\tfrac{\bar M}{2}
        )
        \norm{\tilde{\wth}_1}^2 
        ,
        \end{aligned}
        \label{eq. lya 1}
\end{equation}
where $\mv \Delta:= \mm P\mm B_\xi \mv w(t)$ and $\mm M:= -\mm P\mm B_\xi(\mm I_{l_2}\otimes \estact^\top )-\mm \Lambda{\mm \eta}_1$ are bounded such that $\norm{\mv \Delta}\le\bar\Delta<\infty$ and $\norm{\mm M}_F\le \bar M< \infty$, respectively.

By defining $\mm P=\mm I_n$, the eigenvalues of $\mm Q=-\mm A_\xi^\top -\mm A_\xi$ are $2k_q$ and $2k_z$, since $\mm A_\xi$ is a skew-symmetric matrix except for the diagonal entries.
According to \eqref{eq. lya 1}, if $k_q$ and $k_z$ are provided that
\begin{equation}
    \text{min}(k_q,k_z)>\tfrac{\bar M}{2}
    ,
    \label{eq. ctrl stable condition}
\end{equation}
and if $\lambda_{\theta_1}$ is increased sufficiently large such that $2\lambda_{\theta_1}>\tfrac{\bar M}{2}$, due to the violation of $c_{\theta_1}$, the tracking error is bounded in
\begin{equation}
    \Theta_\xi := 
    \left\{
        \mv\xi\mid
        \norm{\mv\xi}
        \le  
        \tfrac{
            2\bar \Delta
        }{
            \lambda _\text{min}(\mm Q)-{\bar M}
        } 
    \right\}
    ,
\end{equation}
and the weight estimation $\estwth_1$ is bounded in
\begin{equation}
    \Theta_{{\theta}_1} := 
    \left\{ 
        {\estwth_1} 
        \mid
        \norm{\estwth_1}
        \le  
        \bar\theta_1
    \right\}
    .
\end{equation}
The Lagrange multiplier $\lambda_{\theta_1}$ is also bounded since the $\lambda_{\theta_1}$ update stops once ${\estwth}_1$ approaches the compact set $\Theta_{{\theta}_1}$, satisfying the constraint $c_{\theta_1}$.

\subsection*{Step 2: Boundedness of ${\estwth}_0,{\mm \eta}_0$}

The dynamics of ${\mm \eta}_0$ and ${\estwth}_0$ are represented as
\begin{equation}
    \begin{aligned}     
        \ddtt{\mm \eta}_0 =& 
        \mm A_\xi{\mm \eta}_0 -\mm B_\xi 
        \estwV_1^\top \estact'(\mm I_{l_1}\otimes {\q_{n}}^\top )
        \\
        \ddtt{{\estwth}}_0
        =&
        -\alpha 
        (
            {\mm \eta}_0^\top \mm \Lambda\mv \xi+2\lambda_{\theta_0} {\estwth}_0
        )
        .
    \end{aligned}
\end{equation}
According to \cite[Chap.~4 T.~1.9]{Desoer:2009aa}, ${\mm \eta}_0$ is bounded since $\mm A_\xi$ is a stable matrix and $\norm{-\mm B_\xi\estwV_1^\top \estact'(\mm I_{l_1}\otimes {\q_{n}}^\top )}$ is bounded.
To obtain the invariance set of ${\estwth}_0$, taking the time derivative of the Lyapunov function $V_0=\tfrac{1}{2\alpha}{\estwth}_0^\top {\estwth}_0$ yields:
\begin{equation}
    \begin{aligned}
        \ddtt {V}_0 =& 
        -{\estwth}_0^\top (
            {\mm \eta}_0\mm \Lambda\mv \xi 
            +2\lambda_{\theta_0}{\estwth}_0
        )
        \\
        \le &
        \norm{{\estwth}_0} \norm{{\mm \eta}_0\mm \Lambda\mv \xi} 
        -
        2\lambda_{\theta_0} {\estwth}_0^\top {\estwth}_0
        ,
        \\
        \le &
        -2\lambda_{\theta_0} 
        \norm{{\estwth}_0}^2 
        + 
        \norm{{\mm \eta}_0\mm \Lambda\mv \xi}
        \norm{{\estwth}_0}
        .
    \end{aligned}
\end{equation}
Then, the invariance set can be represented as 
\begin{equation}
    \Theta_{{\theta}_0} :=
    \left\{
        {\estwth}_0\mid\norm{\estwth_0}
        \le
        \tfrac{
            \norm{{\mm \eta}_0\mm \Lambda\mv \xi}
        }{
            2\lambda_{\theta_0}
        }
    \right\}
    .    
\end{equation}
If $\lambda_{\theta_0}$ is increased sufficiently large due to the violation of $c_{\theta_0}$, the invariance set $\Theta_{{\theta}_0}$ converges to $\{{\estwth}_0 \mid \norm{{\estwth}_0} \le \bar\theta_0\}$ until the constraint $c_{\theta_0}$ is satisfied.
Therefore, the Lagrange multiplier $\lambda_{\theta_0}$ is also bounded.

\end{proof}

% \begin{remark}
%     In the constrained optimization method, the corresponding method of $L_2$-regularization method is the quadratic penalty method, which replaces the constrained optimization problem into an unconstrained optimization problem by adding the penalty term $(1/2\mu)\sum_{r\in\mathcal I} c_r^2$ in the objective function.
%     The penalty parameter $\mu\in\R_{>0}$ usually decreases over implementation for the convergence of the optimization process.
%     However, the decreased penalty term $\mu$ may alter the original objective function as the penalty term dominates the objective function.
%     Therefore, $L_2$-regularization inherently has the analogous drawback of the quadratic penalty method in the selection of the regularization coefficient $\lambda$.
% \end{remark}

% \subsection{Orthogoanl-regularization}

%  SECTION SIMULATION ======================================
\section{Simulations}\label{sec:sim}

\subsection{Setup}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/RobotModel.drawio.png}
    \caption{Two-link manipulator model.}
    \label{fig: manipulator}
\end{figure}

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{System model parameters.}
    \centering
    \begin{tabular}{c m{11em} c c c }
    \hline
    \textbf{Symbol} & \textbf{Description} & \textbf{Link 1} & \textbf{Link 2} \\
    \hline
    \hline 
    $m_p$ & Mass of $p\textsubscript{th}$ link    & 23.902 (kg) & 3.88 (kg) \\
    \hline
    $l_p$  & Length of $p\textsubscript{th}$ link   & 0.45 (m) & 0.45 (m) \\
    \hline
    ${l_c}_p$ & COM of $p\textsubscript{th}$ link  & 0.091 (m) & 0.048 (m) \\
    \hline
    $b_p$   & Viscous coef. of $p\textsubscript{th}$ link  &  2.288 (Nms) & 0.172 (Nms) \\
    \hline
    ${f_c}_p$  & Friction coef. of $p\textsubscript{th}$ link &  7.17 (Nm) & 1.734 (Nm) \\
    \hline
    \end{tabular}
    \label{table: system parameters}
\end{table}

The two-link manipulator model in \cite{Markus:2013aa} is employed for the simulation demonstration as described in Fig.~\ref{fig: manipulator}.
In the system, the parameters $q_p,{q_d}_p,\tau_p,m_p,l_p,{l_c}_p,b_p$ and ${f_c}_p$ denote the joint angle, desired joint angle, torque, mass, length, center of mass, viscous coefficient, and friction coefficient, respectively, for link $p\in\{1,2\}$.
The values of the system parameters are given in Table~\ref{table: system parameters}.
The reference signal of $\q=[q_1,q_2]^\top $ is defined as follows:
\begin{equation}
    \qd
    =
    \begin{bmatrix}
        {q_d}_1\\
        {q_d}_2
    \end{bmatrix}
    = 
    \begin{bmatrix}
        +\cos(
            \tfrac{\pi}{2}t
        ) + 1 \\
        -\cos(
            \tfrac{\pi}{2}t
        ) - 1 
    \end{bmatrix}
    .
\end{equation}

For the comparative study, three controllers were selected: the neuro–adaptive controller with $L_2$-regularization (NAC-L2) and with $\epsilon$-modification (NAC-eMod) and the proposed controller with constrained optimization (NAC-CO).
%%%%%%%%%%%%%%%%%%%%%
The performances of the selected controllers are compared based on the tracking performances and the dependencies of the parameters (\ie $L_2$ coefficient $\lambda$, $\epsilon$-modification coefficient $\rho$, and $\beta_j$ of NAC-L2, NAC-eMod, and NAC-CO, respectively).
The square root of the integrated squared error (ISE) (\ie $\sqrt{\int_0^T \norm{\mv \xi}^2\,\der t}$, where $T$ denotes a simulation termination time) is utilized to evaluate the tracking performance.
The parameter dependencies of the controllers were examined via various values of the parameters. 
The values ranged from $0.001$ to $1$ across $10$ samples.
%%%%%%%%%%%%%%%%%%%%%

The control laws of all three controllers were the same as those defined in \eqref{eq. control law}.
The adaptation law of NAC-L2 is derived by adding the squared weight term $\tfrac{1}{2}\lambda{\estwth}^\top {\estwth}$ to the objective function such that $J_{L_2}:= J+\tfrac{1}{2}\lambda{\estwth}^\top {\estwth}$, where $\lambda\in\R_{>0}$.
The adaptation law obtained via the gradient descent method is subsequently adjusted by adding a stabilizing term $-\alpha\lambda{\estwth}$ as follows:
\begin{equation} 
    \ddtt{{\estwth}} = 
    \pptfrac{J_{L_2}}{\estwth}
    =-\alpha
    \left(
        \pptfrac{J}{\estwth}
        +
        \lambda{\estwth}
    \right)
    .
    \label{eq. adap L2}
\end{equation}
Note that this adaptation law derived based on $L_2$-regularization method in deep learning is inherently the same as the $\sigma$-modification in adaptive control theory, which adds the term $-\alpha\sigma{\estwth}$, where $\sigma\in\R_{>0}$.
For NAC-eMod, similar to the $\sigma$-modification, the stabilizing function $-\alpha\rho\norm{\tilde{\mv z}}{\estwth}$ is added to the adaptation law as follows:
\begin{equation}
    \ddtt{{\estwth}} = -\alpha
    \left(
        \pptfrac{J}{\estwth}
        +
        \rho\norm{\tilde {\mv z}}{\estwth}
    \right)
    ,
    \label{eq. adap eMod}
\end{equation}
where $\rho\in\R_{>0}$.
By $\norm{\tilde {\mv z}}$, the stabilizing function proportionally increases as the tracking error $\tilde {\mv z}$ increases.
Therefore, the adaptation attempts to reduce the tracking error mainly without the effect of the stabilizing function if the tracking error is sufficiently regulated.
The adaptation law of NAC-CO is presented in \eqref{eq. adaptation laws}.
Owing to the stabilizing functions, the weights of NAC-L2 and NAC-eMod are biased since the stabilizing functions drive the weights toward the origin.

All controllers had the same control parameters except their crucial parameters (\ie $\lambda$, $\rho$ and $\beta_j$) as $k_q=1.1$, $k_z=10$, $\widehat{\mm M}=\mm I_2$ and $\mm \Lambda=\mydiag([5,1,15,15])$.
The parameters of the NNs were set to $l_0=2$, $l_1=16$, $l_2=2$, and $\alpha=10^3$, and the same random seed was applied for weight initialization.
\color{red}    
    The validity of the selected number of nodes were demonstrated through experiments 
\color{black}
The NN input vector was set to the desired trajectory $\qd$, with the augmented 1 to incorporate the bias term in the weight matrix such that $\q_{n}=(\qd^\top ,1)^\top $.
For NAC-CO, the parameters of the weight norm constraints were set as $\bar\theta_0=10$ and $\bar\theta_1=20$.
The sampling time of the simulation and the simulation termination time were set to $T_s=100\,\mu\mathrm{s}$ and $T=10\,\mathrm{s}$, respectively.

\subsection{Results}

\begin{figure}[!t]      
    \centering
    {\includegraphics[width=.85\linewidth]{fig/BoxWhisker.drawio.png}}
\caption{Box-and-Whisker plot of the square root of the tracking ISEs of NAC-L2, NAC-eMod and NAC-CO across various parameter values.}
    \label{fig: Box-beard plot}
\end{figure}

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Quantitative comparison of square root of tracking ISE.}
    \centering
    \begin{tabular}{c c c c }
    \hline
     & \textbf{NAC-L2}\!&\!\textbf{NAC-eMod}\!&\!\textbf{NAC-CO} (proposed) 
     \\
     \hline
    \hline 
    Maximum\!&\!$11.1753\!\times\!10^{-3}$\!&\!$0.5603\!\times\!10^{-3}$\!&\!$0.3439\!\times\!10^{-3}$\!\\
    \hline
    % Upper quartile  & $1.7284$e-3 & $0.5566$e-3 & $0.3261$e-3 \\
    % \hline
    Median\!&\!$0.5898\!\times\!10^{-3}$\!&\!$0.5519\!\times\!10^{-3}$\!&\!$0.3240\!\times\!10^{-3}$\!\\
    \hline
    % Lower quartile  & $0.5533$e-3 & $0.5470$e-3 & $0.3238$e-3 \\
    % \hline
    Minimum\!&\!$0.5434\!\times\!10^{-3}$\!&\!$0.5434\!\times\!10^{-3}$\!&\!$0.3235\!\times\!10^{-3}$\!\\
    \hline
    \end{tabular}
    \label{table: error norm}
\end{table}

As shown in Fig.~\ref{fig: Box-beard plot}, the maximum square root of the tracking ISE of NAC-CO is smaller than the minimum square root of the tracking ISEs of NAC-L2 and NAC-eMod for all variations in the parameters.
This is because NAC-L2 and NAC-eMod bias the weights to the origin due to the presence of stabilizing functions.
A quantitative comparison of the square root of the tracking ISE is provided in Table~\ref{table: error norm}.

\begin{figure}[!t]      
    \centering
        \subfloat[NAC-L2]{\includegraphics[width=.85\linewidth]{fig/fig9.eps}%
        \label{fig: weight NAC-L2}}
    \vspace{.1mm}
        \subfloat[NAC-eMod]{\includegraphics[width=.85\linewidth]{fig/fig10.eps}%
        \label{fig: weight NAC-eMod}}
    \vspace{.1mm}
        \subfloat[NAC-CO]{\includegraphics[width=.85\linewidth]{fig/fig8.eps}%
        \label{fig: weight NAC-CO}}
    \caption{Weight norms of NAC-L2, NAC-eMod, and NAC-CO.}
    \label{fig: weight}
\end{figure}

\begin{figure}[!t]      
    \centering
        \subfloat[NAC-L2]{\includegraphics[width=.85\linewidth]{fig/fig6.eps}%
        \label{fig: error NAC-L2}}
    \vspace{.1mm}
        \subfloat[NAC-eMod]{\includegraphics[width=.85\linewidth]{fig/fig7.eps}%
        \label{fig: error NAC-eMod}}
    \vspace{.1mm}
        \subfloat[NAC-CO]{\includegraphics[width=.85\linewidth]{fig/fig5.eps}%
        \label{fig: error NAC-CO}}
    \caption{Tracking errors of NAC-L2, NAC-eMod, and NAC-CO.}
    \label{fig: error}
\end{figure}

For the detailed analysis, three values of the parameters (\ie $\lambda,\rho,\beta_j\in\{0.001,0.45,1\}$) were selected as described in Fig.~\ref{fig: weight} and Fig.~\ref{fig: error}.
As shown in Fig.~\ref{fig: weight NAC-L2}, increasing $\lambda$ reduces the weight norm of NAC-L2 via the stabilizing function $-\alpha\lambda{\estwth}$.
Moreover, the high dependency of NAC-L2 on the $L_2$-regularization coefficient $\lambda$ can also be observed.
Since the weight norm is decreased, NAC-L2 cannot generate sufficient control inputs, resulting in a larger square root of tracking ISE, as shown in Fig.~\ref{fig: error NAC-L2}.

On the other hand, NAC-eMod has a lower dependency on the $\epsilon$-modification coefficient $\rho$, as shown in Fig.~\ref{fig: weight NAC-eMod} and Fig.~\ref{fig: error NAC-eMod}.
This is because the stabilizing function $-\alpha\rho\norm{\tilde {\mv z}}{\estwth}$ can be decreased once the tracking error $\tilde {\mv z}$ is sufficiently regulated.
However, the bias of the weights to the origin still exists, as described in Fig.~\ref{fig: weight NAC-eMod} (\ie smaller weight norms are observed as $\rho$ increases.).
Therefore, similar to NAC-L2, the biased weights produce insufficient control input, resulting in a relatively larger square root of tracking ISE than that of NAC-CO, as described in Table \ref{table: error norm}.

Finally, the weight norm of NAC-CO is smaller than those of NAC-L2 and NAC-eMod, as shown in Fig.~\ref{fig: weight NAC-CO}, with better tracking performance.
Even if a large $\beta_j$ is provided, NAC-CO can adjust the adaptation direction to satisfy the weight norm constraints faster, according to \eqref{eq. adaptation law L}.
Therefore, the lowest dependency on the update rate $\beta_j$ is observed in NAC-CO, as shown in Fig.~\ref{fig: weight NAC-CO} and Fig.~\ref{fig: error NAC-CO}.
Note that $\beta_j$ of NAC-CO is the update rate for the Lagrange multipliers, whereas $\lambda$ and $\rho$ are the coefficients of the stabilizing function that generates the biases of the weights.
However, considering the implementation using a digital computer, excessively large $\beta_j$ values should be avoided.

The details of the satisfaction of the weight norm constraints are shown in Fig.~\ref{fig: weight and multiplier} for NAC-CO with $\beta_j=0.001$.
As the weight norms of each layer reach the constraint boundary, the corresponding Lagrange multipliers are generated.
Using the Lagrange multipliers, the adaptation direction is adjusted toward the constraint satisfactory point.
The Lagrange multipliers disappear when the constraints are satisfied, and the weights are adapted to optimize the original objective function without weight bias.

\begin{figure}[!t]      
    \centering
        \subfloat[Weight norm]{\includegraphics[width=.8 \linewidth]{fig/fig12.eps}%
        \label{fig: weight per layer NAC-CO}}
    \vspace{.1mm}
        \subfloat[Lagrange multipliers]{\includegraphics[width=.8 \linewidth]{fig/fig11.eps}%
        \label{fig: multipliers NAC-CO}}
        \caption{Weight norms and Lagrange multipliers of NAC-CO ($\beta=0.001$).}
        \label{fig: weight and multiplier}
\end{figure}

Furthermore, it is important to note that NAC-CO shows enhanced tracking performance with smaller weights than NAC-L2 and NAC-eMod.
This implies that the weights in NAC-CO approach the different local optimal solution points from those of NAC-L2 and NAC-eMod.
Therefore, if the physical analysis of the system is available to predict the feasible maximum control inputs, NAC-CO can find the local optimal solution without unnecessarily large control inputs by imposing proper weight norm constraints.


%  SECTION CONCLUSION ======================================
\section{Conclusion}\label{sec:conclusion}

%
In this paper, a neuro–adaptive control method is proposed for uncertain Euler‒Lagrange systems, ensuring weight boundedness.
Adaptation laws are derived by formulating a constrained optimization problem with weight norm constraints.
The boundedness of the tracking error and the weight estimation are analyzed via Lyapunov analysis.
The simulation results demonstrate that the proposed controller outperforms the existing methods in terms of tracking performance and parameter dependency.
As further work, the state constraints for safety will be handled, ensuring stability.
%

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\footnotesize
    \bibliographystyle{ieeetr}
    \bibliography{dding_template/refs}
}

\end{document}

